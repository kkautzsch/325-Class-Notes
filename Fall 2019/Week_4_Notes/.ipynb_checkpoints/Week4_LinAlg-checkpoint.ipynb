{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS 325 Scientific Computing -- Fall 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Numerical methods\n",
    "\n",
    "*Acknowledgements:* My lecture notes for this chapter draw from the \"Numerical Mathematics for Physicists\" course taught by [Martin Kerscher](https://homepages.physik.uni-muenchen.de/~Martin.Kerscher/), as well as lectures by [Lode Pollet](https://www.theorie.physik.uni-muenchen.de/lsschollwoeck/members/professors/pollet/) and the \"Numerical Recipes\" books.\n",
    "\n",
    "## 2.1 Linear algebra\n",
    "\n",
    "### 2.1.1 Solving a system of linear equations\n",
    "\n",
    "Simple system of three linear equations\n",
    "\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "x_1 & + 5\\ x_2  & +7\\ x_3  & = \\ \\ 1 \\\\\n",
    "    & - 15\\ x_2 & -17\\ x_3 & =-1 \\\\\n",
    "    &          & -10\\ x_3 & =-2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- How do we solve such a system?\n",
    "- What property makes it relatively easy to solve?\n",
    "\n",
    "The goal is to make any system of linear equations to look like the one above.\n",
    "\n",
    "Example from Warm-Up:\n",
    "\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "x_1     & + 5\\ x_2   & +7\\ x_3 & = 1 \\\\\n",
    "3\\ x_1  &            & +4\\ x_3 & = 2 \\\\\n",
    "7\\ x_1  &  + 5\\ x_2  & +5\\ x_3 & = 3\n",
    "\\end{array} .\n",
    "$$\n",
    "\n",
    "Or the same in matrix notation:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "  1 & 5 & 7 \\\\\n",
    "  3 & 0 & 4 \\\\\n",
    "  7 & 5 & 5 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  x_1\\\\ x_2\\\\ x_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  1\\\\ 2\\\\ 3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- What operations are we allowed to do?\n",
    "\n",
    "> we will go through this example on the board\n",
    "\n",
    "**General** system of linear equations:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}\n",
    "  a_{11} x_1 + a_{12} x_2 + \\cdots + a_{1N} x_N & = b_1 \\\\\n",
    "  \\vdots & \\vdots \\\\\n",
    "  a_{M1} x_1 + a_{M2} x_2 + \\cdots + a_{MN} x_N & = b_M\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$M$ equations fÃ¼r $N$ unknowns ($x_1,\\ldots,x_N$)\n",
    "\n",
    "- numerics: typically thousands of variables\n",
    "- for linear problems or linear approximations to other problems\n",
    "- needed for many numerical setups (for example splines etc.)\n",
    "- many highly optimized libraries available\n",
    "\n",
    "**Matrix notation**:\n",
    "\n",
    "$$\n",
    "A  \\mathbf{x} = \\mathbf{b} \n",
    "$$\n",
    "\n",
    "where $A$ is an $M \\times N$ matrix, and $\\mathbf{x}$ and $\\mathbf{b}$ are column vectors of size $N$ and $M$, respectively:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "  a_{11} & \\cdots & a_{1N} \\\\ \n",
    "  \\vdots &  & \\vdots \\\\ \n",
    "  a_{M1} & \\cdots & a_{MN}\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "  x_{1} \\\\ \n",
    "  \\vdots  \\\\ \n",
    "  x_{N} \n",
    "\\end{pmatrix} \n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  b_{1} \\\\ \n",
    "  \\vdots  \\\\ \n",
    "  b_{M} \n",
    "\\end{pmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(From now on we will mostly consider quadratic matrices, so $M=N$, unless stated otherwise)\n",
    "\n",
    "Solution using **Gaussian elimination**, schematically:\n",
    "\n",
    "$\\ $ | $\\ $  | $\\ $ | $\\ $ | $\\ $\n",
    "---|---|----|----|----\n",
    "![start](images/Kap4Gauss0.png) | $\\longrightarrow$ | ![step 1](images/Kap4Gauss1.png) | $\\longrightarrow$ |![step 2](images/Kap4Gauss2.png) \n",
    "\n",
    "End result: **triangular form**\n",
    "\n",
    "![triangular](images/Kap4GaussN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step by step:\n",
    "\n",
    "- the goal is to bring the matrix to upper triangular form $U$\n",
    "- we proceed column by column, starting from the left\n",
    "- We are allowed to:\n",
    "    - swap rows\n",
    "    - multiply a row by a (non-zero) constant\n",
    "    - add a multiple of one row to another row\n",
    "- let's assume we have already finished the first $n-1$ columns\n",
    "- this means that all their elements below the diagonal are 0\n",
    "- to keep track of where we are, we will call our transformed matrix after $n-1$ steps $A^{(n-1)}$ and its elements $a_{ij}^{(n-1)}$ (where $i$ stands for the row and $j$ for the column)\n",
    "- as an example, look at the matrix $A^{(2)}$ (after 2 steps) in the schematic above\n",
    "- another way of saying that the elements of column $n-1$ are zero below the diagonal is $a_{i,n-1}^{(n-1)} = 0$ for $i>n-1$\n",
    "- now we transform column $n$ in step $n$\n",
    "- the first $n$ rows are already done, but we need to make all elements below the diagonal, $a_{in}^{(n)} = 0$ for $i>n$, equal zero\n",
    "- to achieve this, we subtract from each of the rows *below row n* a multiple of row $n$, which is the uppermost row that already has the correct form\n",
    "- we need to pick each multiple $l_{in}$ such that we get the correct zeros: $l_{in} = \\frac{a_{in}^{(n-1)}}{a_{nn}^{(n-1)}}$\n",
    "- then the subtraction yields $a_{ij}^{(n)} = a_{ij}^{(n-1)} - l_{in}a_{nj}^{(n-1)}$ for $i>n$ and $j>n$\n",
    "- careful: this only works if $a_{nn}^{(n-1)}\\neq0$ (we will get back to this later) \n",
    "- once we have the multiple, we make the subtraction for all the remaining rows, both of the matrix and of the vector $\\mathbf{b}$\n",
    "- repeat until finished!\n",
    "\n",
    "<br>\n",
    "<tr>\n",
    "<td><img src=\"images/infmanysolutions.png\" alt=\"Infinitely many solutions\" align=\"right\"  style=\"width: 200px;\"/></td>\n",
    "<td><img src=\"images/uniquesolution.png\" alt=\"Infinitely many solutions\" align=\"right\"  style=\"width: 200px;\"/></td>\n",
    "<td><img src=\"images/nosolution.png\" alt=\"Unique solution\" align=\"right\" style=\"width: 200px;\"/></td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "#### Possible cases\n",
    "- no solution\n",
    "- unique solution\n",
    "- infinitely many solutions\n",
    "\n",
    "<br><br><br><br><br><br>\n",
    "For quadratic matrices:\n",
    "\n",
    "<center>$A$ is invertible</center>\n",
    "$$\\Leftrightarrow$$\n",
    "<center>rank($A$)=$N$ </center>\n",
    "$$\\Leftrightarrow$$\n",
    "<center>det$(A)\\neq0$</center>\n",
    "$$\\Leftrightarrow$$\n",
    "<center>$A\\mathbf{x}=0$ has only the solution $\\mathbf{x}=0$ </center>\n",
    "\n",
    "<br>\n",
    "Formally\n",
    "\n",
    "$$A^{-1}\\mathbf{b}=\\mathbf{x}$$\n",
    "\n",
    "so solving a system of linear equations is related to inverting a matrix!\n",
    "\n",
    "> never use a direct implementation of matrix inversion => slow and numerically unstable\n",
    "\n",
    "(but you can do the opposite: solve $A\\mathbf{x}_j=\\mathbf{e}_j$ for each unit vector $\\mathbf{e}_j$ to obtain the inverse)\n",
    "\n",
    "#### Tasks of Computational Linear Algebra\n",
    "\n",
    "- solving sets of linear equations\n",
    "- matrix determinants\n",
    "- matrix inversion\n",
    "- singular value decomposition of a matrix\n",
    "- linear least squares => see section on data fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Numerical problems\n",
    "\n",
    "- equations may formally have a unique solution, but some of the equations may be close to being linearly dependent<br> => roundoff errors in the solution process can make them linearly dependent <br>=> failure of the solution procedure\n",
    "- large $N$: roundoff errors may swamp the true solution <br>=> numerical instability <br>=> wrong solution (need to check!)\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "  \\begin{pmatrix}\n",
    "    10 & 7 & 8 & 7 \\\\\n",
    "    7 & 5 & 6 & 5 \\\\\n",
    "    8 & 6 & 10 & 9 \\\\\n",
    "    7 & 5 & 9 & 10\\\\\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    x_1\\\\ x_2\\\\ x_3\\\\ x_4\n",
    "  \\end{pmatrix} = \n",
    "  \\begin{pmatrix}\n",
    "    32\\\\ 23\\\\ 33\\\\ 31\n",
    "  \\end{pmatrix}\\Rightarrow\n",
    "  \\mathbf{x}=  \\begin{pmatrix}\n",
    "    1\\\\ 1\\\\ 1\\\\ 1\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Very similar system:\n",
    "\n",
    "$$\n",
    "  \\begin{pmatrix}\n",
    "    10 & 7 & 8 & 7 \\\\\n",
    "    7 & 5 & 6 & 5 \\\\\n",
    "    8 & 6 & 10 & 9 \\\\\n",
    "    7 & 5 & 9 & 10\\\\\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    x_1\\\\ x_2\\\\ x_3\\\\ x_4\n",
    "  \\end{pmatrix} = \n",
    "  \\begin{pmatrix}\n",
    "    32.1\\\\ 22.9\\\\ 33.1\\\\ 30.9\n",
    "  \\end{pmatrix}\\Rightarrow\n",
    "  \\mathbf{x}=  \\begin{pmatrix}\n",
    "    9.2\\\\ -12.6\\\\ 4.5\\\\ -1.1\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- a relative error of $0.1/23 \\approx 0.00434$ in $\\mathbf{b}$ results in a relative error of $12.6/1$ on the result\n",
    "- relative error enhancement factor $\\approx3000$.\n",
    "- general problem in systems of linear equations, independent of numerical method\n",
    "\n",
    "This problem can be quantified: **condition number**\n",
    "\n",
    "$$\\kappa (A) = \\Vert A^{-1} \\Vert \\Vert A \\Vert$$\n",
    "\n",
    "Here $\\Vert A \\Vert$ is a **matrix norm**. There are many different matrix norms, for example\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\Vert A \\Vert _{\\rm rows} &:= \\max_i \\left( \\sum_k \\vert a_{ik} \\vert \\right)\\\\  \n",
    "\\Vert A \\Vert _{\\rm cols} &:= \\max_k \\left( \\sum_i \\vert a_{ik} \\vert \\right) \\\\  \n",
    "\\Vert A \\Vert _{\\rm Frobenius} &:= \\left( \\sum_i \\sum_k a_{ik}^2 \\right) ^{\\frac{1}{2}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- which matrix norm exactly we use doesn't matter => only order of magnitude matters\n",
    "- the larger the condition number of the matrix the larger the relative error enhancement\n",
    "\n",
    "Using the [NumPy Linear Algebra submodule](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.linalg.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  7,  8,  7],\n",
       "       [ 7,  5,  6,  5],\n",
       "       [ 8,  6, 10,  9],\n",
       "       [ 7,  5,  9, 10]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([10, 7, 8, 7, 7, 5, 6, 5, 8, 6, 10, 9, 7, 5, 9, 10])\n",
    "b = a.reshape((4, 4))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.54504869860253"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "LA.norm(b)          # Frobenius norm by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3009.578708058694"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.norm(b)*LA.norm(LA.inv(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3009.578708058694"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.cond(b,'fro')    # condition number directly, specifying Frobenius norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational complexity of Guassian elimination:\n",
    "\n",
    "- step $n$ of the process\n",
    "     - $N-n$ divisions for $l_{in}$\n",
    "     - $2(N-n)^2$ additions and multiplications for $a_{ij}^{(n)}$\n",
    "     - $2(N-n)$ additions and multiplications for $b_i^{(n)}$\n",
    "     \n",
    "  Altogether ($i=N-n$):\n",
    "  \n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\#\\text{FLOPs} & = 3 \\sum_{n = 1}^{N-1} (N - n) + 2 \\sum_{n = 1}^{N-1}(N - n)^2 \\\\\n",
    "  & = \\frac{2}{3} N^3 - \\frac{1}{6} N^2 -\\frac{1}{2} N \n",
    "  \\end{align*}\n",
    "  $$\n",
    "- solving for each $x_i$ in the end\n",
    "    - $2(N-(i+1))$ additions and multiplications in the sum\n",
    "    - one multiplication and one addition outside the sum\n",
    "    - one multiplication for $x_N$\n",
    "    \n",
    "  Altogether:\n",
    "  \n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\#\\text{FLOPs} & =  1 + \\sum_{i = 1}^{N-1} ( 2(N-i-1)\\ +\\ 2) \\\\\n",
    "  & =  N^2 -N + 1\n",
    "  \\end{align*}\n",
    "  $$\n",
    "- memory and element access is also very important for matrix operations<br>\n",
    "  => beyond the scope of this lecture\n",
    "\n",
    "In total:\n",
    "\n",
    "$$\n",
    "\\#\\text{FLOPs} = \\frac{2}{3} N^3 + \\frac{5}{6} N^2 - \\frac{3}{2} N + 1 =\\mathcal{O}(N^3)\n",
    "$$\n",
    "\n",
    "### 2.1.3 LU decomposition\n",
    "\n",
    "- very similar to Gaussian elimination\n",
    "- the main idea is to keep track of all Gaussian elimination steps, not just the end result\n",
    "- this saves us time if we need to solve several similar systems of linear equations\n",
    "- the Gaussian elimination steps are remembered by writing them into a matrix\n",
    "\n",
    "$$\n",
    "  A = L U \n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    1 &   &  &  0 \\\\ \n",
    "    l_{21} & 1 &  &  \\\\ \n",
    "    \\vdots &  \\ddots & \\ddots &  \\\\ \n",
    "    l_{N1} & \\cdots & l_{NN-1} & 1\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    u_{11} & u_{12} & \\cdots & u_{1N} \\\\ \n",
    "    & u_{22} &  & \\vdots \\\\ \n",
    "    &  & \\ddots & \\vdots \\\\ \n",
    "    0&  &  & u_{NN}\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- same steps as Gaussian elimination for $U$\n",
    "- we have already computed the elements of $L$ along the way\n",
    "\n",
    "Solving a set of linear equations:\n",
    "- first solve $L \\mathbf{y} = \\mathbf{b}$ for $\\mathbf{y}=U \\mathbf{x}$ \n",
    "- then solve $U \\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$ as before\n",
    "- if we *already have* the LU decomposition we need $2(N^2-N+1)$ FLOPs to solve the set of equations for *any* $\\mathbf{b}$, so $\\mathcal{O}(N^2)$\n",
    "- we get the determinant (almost) for free:\n",
    "\n",
    "  $$\\det(A) = \\det(LU) = \\det(L)\\det(U) = \\prod_{i=1}^{N}u_{ii}$$\n",
    "\n",
    "### 2.1.4 Pivoting\n",
    "\n",
    "- so far we have always assumed $a_{nn}^{(n-1)}\\neq0$, so that we can calculate $l_{in} = \\frac{a_{in}^{(n-1)}}{a_{nn}^{(n-1)}}$\n",
    "- if this is not the case => swap rows!\n",
    "- swapping rows can be described by a permutation matrix $P$, for example:\n",
    "\n",
    "  $$\n",
    "  \\begin{pmatrix}\n",
    "    0&1&0\\\\\n",
    "    0&0&1\\\\\n",
    "    1&0&0\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}x_2\\\\x_3\\\\x_1\\end{pmatrix}\n",
    "  $$\n",
    "- then we need to solve $PA\\mathbf{x}=P\\mathbf{b}$ and decompose $PA=LU$\n",
    "\n",
    "The matrix element $a_{nn}^{(n-1)}$ is called **pivot element**\n",
    "- even though we can choose $a_{nn}^{(n-1)} \\neq 0$ it can happen that it is very close to 0 <br>=> numerical problems\n",
    "- subtractions in every step, so we can lose significant digits, e.g. in\n",
    "\n",
    "  $$\n",
    "  a_{ik}^{(1)}=a_{ik}^{(0)}-l_{i1} a_{1k}^{(0)}\n",
    "  $$\n",
    "- for example, if we have $|a_{nn}^{(1)}|\\ll1$ then $l_{in}$ is very large and the roundoff error from before is amplified in\n",
    "\n",
    "  $$\n",
    "  a_{ik}^{(2)} = a_{ik}^{(1)} -\n",
    "  \\overset{\\text{roundoff error}}{\\underset{\\text{very large}}\n",
    "    {\\underset{\\uparrow}{l_{i2}\\ } \\overset{\\downarrow}{a_{2k}^{(1)}}}}\n",
    "  $$\n",
    "\n",
    "=> pivoting is **essential** for numerical stability!\n",
    "\n",
    "**Column maximization strategy**\n",
    "- in every step of Gaussian elimination swap the remaining rows until\n",
    "\n",
    "  $$\n",
    "  \\vert a_{nn}^{(n-1)} \\vert = \\max_{i \\geq n} \\vert a_{in}^{(n-1)} \\vert \n",
    "  $$\n",
    "- in words: swap until absolute value of the element $a_{nn}^{(n-1)}$ is the largest possible\n",
    "- further improvements on column maximization possible\n",
    "- book-keeping over raw swaps in terms of the permutation matrix\n",
    "\n",
    "Extreme example:\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "  \\epsilon & 1 \\\\\n",
    "  1        & 1 \n",
    "\\end{pmatrix}\n",
    "= L U =\n",
    "\\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  \\epsilon^{-1} & 1 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  \\epsilon & 1 \\\\\n",
    "   0 & 1 - \\epsilon^{-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "After pivoting:\n",
    "\n",
    "$$\n",
    "PA = A' = \n",
    "\\begin{pmatrix}\n",
    "  1        & 1 \\\\\n",
    "  \\epsilon & 1 \n",
    "\\end{pmatrix}\n",
    "= L' U' =\n",
    "\\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  \\epsilon & 1 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "   1 & 1 \\\\\n",
    "   0 & 1 - \\epsilon\n",
    "\\end{pmatrix} .\n",
    "$$\n",
    "\n",
    "Assume $\\epsilon<\\epsilon_m$ (machine precision), then\n",
    "\n",
    "$$1-\\epsilon^{-1}\\rightarrow-\\epsilon^{-1}$$\n",
    "and the component $u_{22}$ of the matrix $U$ is not exact. Instead of $LU$ we get\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  \\epsilon^{-1} & 1 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "   1 & 1 \\\\\n",
    "   0 & -\\epsilon^{-1}\n",
    "\\end{pmatrix} \n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  \\epsilon & 1 \\\\\n",
    "  1 & 0 \n",
    "\\end{pmatrix}\n",
    "\\ne A.\n",
    "$$\n",
    "\n",
    "**Special cases**:\n",
    "- band diagonal matrices (especially tri-diagonal => spline interpolation)\n",
    "    - only need to save diagonals (memory saving)\n",
    "    - not solved with LU decomposition, but for each $\\mathbf{b}$ with Gaussian elimination ($\\mathcal{O}(N)$) for this special case\n",
    "    - usually no pivoting necessary\n",
    "- symmetric matrices => Cholesky decomposition\n",
    "    - similar to LU decomposition\n",
    "    - no pivoting necessary\n",
    "    - also some memory saving\n",
    "- existence and uniqueness of LU decomposition\n",
    "    - LU decompositions are (in general) not unique\n",
    "    - LU decompositions do not always exist\n",
    "    - a square matrix always has an LU decomposition **with pivoting** (LUP decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Matrix inversion\n",
    "\n",
    "> essentially the same as solving sets of linear equations\n",
    "\n",
    "Assume $\\det(A) \\neq 0$ and we have a way to solve a system of linear equations $A \\mathbf{x} =\\mathbf{b}$ (e.g. LU decomposition)\n",
    "\n",
    "To obtain $A^{-1}$:\n",
    "- solve the linear equation $A \\mathbf{x}_i = \\mathbf{e}_i$ for all unit vectors $\\mathbf{e}_i$ and obtain the $\\mathbf{x}_i$\n",
    "- then\n",
    "\n",
    "  $$\n",
    "  \\mathbf{x}_i = A^{-1} \\mathbf{e}_i = \n",
    "  \\begin{pmatrix}\n",
    "    (a^{-1})_{11} & \\cdots & (a^{-1})_{1N} \\\\\n",
    "    &&\\\\ \n",
    "    \\vdots &  & \\vdots \\\\ \n",
    "    &&\\\\\n",
    "    (a^{-1})_{N1} & \\cdots & (a^{-1})_{NN}\n",
    "  \\end{pmatrix}  \n",
    "  \\begin{pmatrix}\n",
    "    0 \\\\ \n",
    "    \\vdots \\\\ \n",
    "    1 \\\\ \n",
    "    \\vdots \\\\ \n",
    "    0\n",
    "  \\end{pmatrix} =\n",
    "  \\begin{pmatrix}\n",
    "    (a^{-1})_{1i} \\\\ \n",
    "    \\\\ \n",
    "    \\vdots \\\\ \n",
    "    \\\\ \n",
    "    (a^{-1})_{Ni}\n",
    "  \\end{pmatrix}  \n",
    "  $$\n",
    "  \n",
    "  meaning that the $\\mathbf{x}_i$ are the column vectors of $A^{-1}$:\n",
    "  \n",
    "  $$\n",
    "  A^{-1} = ( \\mathbf{x}_1 , \\mathbf{x}_2 , \\ldots , \\mathbf{x}_N ) .\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Linear algebra libraries\n",
    "\n",
    "Equally important:\n",
    "- small number of operations\n",
    "- small memory requirement\n",
    "- clever memory read-out\n",
    "\n",
    "#### Column and row major\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "How is a matrix stored in computer memory?\n",
    "\n",
    "- row major (C, C++, Python): $\\ \\ \\ \\ $ contiguously in memory as ```1 2 3 4 5 6```\n",
    "- column major (Fortran, Matlab): contiguously in memory as ```1 4 2 5 3 6```\n",
    "\n",
    "(this generalizes to higher dimensions)\n",
    "\n",
    "A basis for many modern libraries is the Basic Linear Algebra Subsystem ([BLAS](http://www.netlib.org/blas/)):\n",
    "- 3 levels:\n",
    "    - level 1: scalar, scalar-vector and vector-vector\n",
    "    - level 2: matrix-vector\n",
    "    - level 3: matrix-matrix\n",
    "- huge increase in speed possible\n",
    "- free implementations: MKL, ATLAS,...\n",
    "- there are also machine specific optimized BLAS libraries (e.g. AMD: ACML, Apple: Accelerate, Intel: MKL)\n",
    "- here is the [documentation](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms)\n",
    "\n",
    "Some libraries and modules:\n",
    "- [LAPACK](http://www.netlib.org/lapack/) uses BLAS, especially Level 3\n",
    "- [GSL](https://www.gnu.org/software/gsl/) uses BLAS\n",
    "- Mathematica, Matlab and Octave use BLAS\n",
    "- NumPy and [SciPy](https://docs.scipy.org/doc/scipy/reference/linalg.html) use BLAS\n",
    "- you can also call BLAS functions in SciPy directly\n",
    "\n",
    "```scipy.linalg``` vs ```numpy.linalg```:\n",
    "- ```scipy.linalg``` contains all functions in ```numpy.linalg``` plus some more advanced ones\n",
    "- ```scipy.linalg``` is always compiled with BLAS/LAPACK support, while for NumPy this is optional<br> => SciPy might be faster depending on how your NumPy was installed\n",
    "\n",
    "> Use ```scipy.linalg``` unless you don't want to import ```scipy```\n",
    "\n",
    "Implementation on GPUs (graphics processing units)\n",
    "- signficant improvement in GPUs, thanks to the gaming industry\n",
    "- multicore GPUs are now affordable\n",
    "- massively parallel\n",
    "- libraries specific for GPUs (cuBLAS, Magma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.linalg as LA\n",
    "\n",
    "myMatrix = scipy.array([[1., 2.], [3., 4.]])\n",
    "\n",
    "# matrix inversion\n",
    "myInvMatrix = LA.inv(myMatrix)\n",
    "print(myInvMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   0.00000000e+00]\n",
      " [  8.88178420e-16   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# check that the inverse is correct\n",
    "print(scipy.dot(myMatrix, myInvMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.around(scipy.dot(myMatrix, myInvMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix determinant\n",
    "LA.det(myMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0000000000000013"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1./LA.det(myInvMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 Iterative improvement\n",
    "\n",
    "- let $\\mathbf{x}$ be the exact solution of $A \\mathbf{x} = \\mathbf{b}$ (which we don't know)\n",
    "- let $\\hat{\\mathbf{x}} = \\mathbf{x} +\\delta \\mathbf{x}$ be the numerical (non-exact) solution  \n",
    "\n",
    "Then\n",
    "\n",
    "$$ \n",
    "A \\hat{\\mathbf{x}} = A \\mathbf{x} + A \\delta \\mathbf{x} = \\mathbf{b} + \\mathbf{r} \n",
    "$$\n",
    "\n",
    "with the **residual**\n",
    "\n",
    "$$\n",
    "\\mathbf{r} = A \\delta \\mathbf{x} = A \\hat{\\mathbf{x}} - \\mathbf{b} \n",
    "$$\n",
    "\n",
    "After calculating the LU decomposition with a **direct method** we know\n",
    "- $A$\n",
    "- $LU$ (not exactly the same as $A$ because of numerical errors!)\n",
    "- $\\mathbf{b}$\n",
    "- $\\hat{\\mathbf{x}}$ (our numerical non-exact solution)  \n",
    "\n",
    "Algorithm for iterative improvement:\n",
    "\n",
    "- compute $\\mathbf{r} = A \\hat{\\mathbf{x}} - \\mathbf{b}$\n",
    "- calculate $\\delta \\mathbf{x}$ as solution of $A \\delta \\mathbf{x} = \\mathbf{r}$\n",
    "  (since we already know $A = LU$ this is $\\mathcal{O}(N^2)$)\n",
    "- compute the improved solution\n",
    "\n",
    "  $$ \n",
    "  \\mathbf{x}_{\\mathrm{new}} = \\hat{\\mathbf{x}} - \\delta \\mathbf{x}\n",
    "  $$\n",
    "  \n",
    "- this can be repeated\n",
    "- (there are also iterative methods to improve the LU decomposition itself => not covered here)\n",
    "\n",
    "Additional cost:\n",
    "\n",
    "- the residue $\\mathbf{r}=A \\hat{\\mathbf{x}} - \\mathbf{b}$ has to be calculated\n",
    "- both $A$ and $LU$ have to be kept in memory\n",
    "- additional computation time $\\mathcal{O}(N^2)$ per iteration step is negligible compared to the $\\mathcal{O}(N^3)$ of the direct method (for sufficiently large matrices)\n",
    "- iterative methods are useful for very large $N$ and if we can already find a good approximation of $\\mathbf{x}$ with  $k \\ll N$ iterations (only $k N^2 \\ll N^3$ FLOPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 Eigenvalue problems\n",
    "\n",
    "Definition eigenvalue $\\lambda$ and eigenvector $\\mathbf{v}$ of square matrix $A$:\n",
    "\n",
    "$$A\\mathbf{v}=\\lambda\\mathbf{v}$$\n",
    "\n",
    "Eigenvectors only get \"stretched\" by the matrix, their direction doesn't change!\n",
    "\n",
    "Very common problems in physics (and science in general):\n",
    "\n",
    "- needed to solve ordinary differential equations => later in the lecture\n",
    "- quantum mechanics (time-independent SchrÃ¶dinger equation => atomic orbitals, energy eigenstates)\n",
    "- statistics, analysis of large data sets\n",
    "- vibration analysis\n",
    "- mechanics and solid mechanics (stress tensor, moment of inertia tensor)\n",
    "- facial recognition\n",
    "\n",
    "Properties:\n",
    "\n",
    "- multiples of an eigenvector don't count as a new eigenvector\n",
    "- 0 is not an eigenvector of any matrix (but an eigenvalue can equal 0!)\n",
    "- an $N\\times N$ matrix has $N$ eigenvectors with corresponding (in general complex) eigenvalues\n",
    "- some of these eigenvalues can be identical\n",
    "- eigenvectors are orthogonal to each other: $\\mathbf{v}_i\\cdot\\mathbf{v}_j=0$ if $i\\neq j$\n",
    "\n",
    "We can also write all eigenvectors and eigenvalues into matrices:\n",
    "\n",
    "$$AV=VD,\\ \\ \\ {\\rm where}\\ V=(v_1 \\ldots v_N),\\ \\ \\ {\\rm and}\\ D=\n",
    "\\begin{pmatrix}\n",
    "\\lambda_1 & \\cdots & 0\\\\\n",
    " & \\ddots & \\\\\n",
    "0 & \\cdots & \\lambda_N\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Normalized (and orthogonal) eigenvectors $\\Rightarrow VV^T=V^TV=\\mathbf{1}$\n",
    "\n",
    "Calculating eigenvalues \"by hand\" often by solving the equation\n",
    "\n",
    "$$\\det (A-\\lambda\\mathbf{1}) = 0$$\n",
    "\n",
    "The left hand side is the **characteristic polynomial**\n",
    "\n",
    "=> remember: a polynomial of degree $N$ can always be factored into the product of $N$ terms:\n",
    "\n",
    "$$\\det(A â \\lambda\\mathbf{1}) = (\\lambda _{1}-\\lambda )(\\lambda _{2}-\\lambda )\\cdots (\\lambda _{n}-\\lambda )$$\n",
    "\n",
    "- computationally finding the roots of the polynomial is inefficient (see next section)\n",
    "- instead there are many more efficient algorithms, for example\n",
    "    - QR algorithm, based on writing the matrix as a product of an orthogonal and an upper triangular matrix\n",
    "    - Jacobi algorithm for symmetric matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.37228132+0.j  5.37228132+0.j] \n",
      "\n",
      "[[-0.82456484 -0.41597356]\n",
      " [ 0.56576746 -0.90937671]]\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.linalg as LA\n",
    "\n",
    "myMatrix = scipy.array([[1., 2.], [3., 4.]])\n",
    "eigenValues, eigenVectors = LA.eig(myMatrix)\n",
    "print(eigenValues,'\\n')\n",
    "print(eigenVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.30697009 -0.21062466]\n",
      "[ 0.30697009-0.j -0.21062466+0.j]\n"
     ]
    }
   ],
   "source": [
    "print(scipy.dot(myMatrix,eigenVectors[:,0]))\n",
    "print(eigenValues[0]*eigenVectors[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.9 Singular Value Decomposition (SVD)\n",
    "\n",
    "- powerful method for singular or near-singular systems\n",
    "- can \"diagnose\" problems with LU decomposition\n",
    "- useful for \"inverting\" non-square matrices, determining the matrix rank etc.\n",
    "- needed for data analysis (linear least-squares) => later in the lecture\n",
    "- has applications in machine learning, artificial intelligence, image compression,...\n",
    "\n",
    "Any $M\\times N$ matrix $A$ (no matter how singular!) can be written as a product of three matrices like this:\n",
    "\n",
    "$$A=U\\Sigma V^T$$\n",
    "\n",
    "with \n",
    "- an $M\\times N$ column orthogonal matrix $U$,\n",
    "- an $N\\times N$ diagonal matrix $\\Sigma$ (diagonal elements $\\sigma_{i}\\geq0$ are the **singular values**)\n",
    "- an $N\\times N$ orthogonal matrix $V^T$\n",
    "\n",
    "Schematically\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "& & & & \\\\\n",
    "& &   & & \\\\\n",
    "& & {A}   & & \\\\\n",
    " & &  & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & &\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& &   & & \\\\\n",
    "& & {U}  & & \\\\\n",
    " & &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & &\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\!\\sigma_1\\! &  & & \\\\\n",
    " &\\!\\sigma_2\\!  & & \\\\\n",
    " &   &\\!\\ddots\\! & \\\\\n",
    "  &   & &\\!\\sigma_N\\!\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& &\\!{V}^T\\!  & & \\\\\n",
    "& &   & & \\\\\n",
    " & &   & & \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Orthogonality conditions, schematically:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "& & &  & & & & & \\\\\n",
    "& & &  & {U}^T & & & \\\\\n",
    " & & & & & & & & \\\\\n",
    "& &  & & & &\n",
    "\\end{pmatrix}\\cdot\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& &   & & \\\\\n",
    "& & {U}  & & \\\\\n",
    " & &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & &\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& &\\!{V}^T\\! & & \\\\\n",
    "& &   & & \\\\\n",
    " & &   & & \n",
    "\\end{pmatrix}\\cdot\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& & {V}  & & \\\\\n",
    "& &   & & \\\\\n",
    " & &   & & \n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\! 1&  & & \\\\\n",
    " & \\!1  & & \\\\\n",
    " &   &\\! \\ddots& \\\\\n",
    " &   & &\\!1 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Visualization of SVD in 2D:\n",
    "- start from disc with 2 unit vectors\n",
    "- in this example, the original matrix distorts the disc to an ellipse (a matrix is a *linear transformation*)\n",
    "- SVD decomposes the matrix into three simple transformations: \n",
    "     - an initial rotation $V^â$ (this is the conjugate transpose; since we work in real numbers, we use the notation $V^T$ for regular transpose), \n",
    "     - a scaling $\\Sigma$ along the coordinate axes\n",
    "     - a final rotation $U$\n",
    "- The lengths $\\sigma_1$ and $\\sigma_2$ of the semi-axes of the ellipse are the singular values\n",
    "\n",
    "![SVD illustration](images/Singular_value_decomposition.gif)\n",
    "\n",
    "Image from [Wikipedia](https://en.wikipedia.org/wiki/Singular-value_decomposition)\n",
    "\n",
    "<img src=\"images/Singular_value_decomposition.png\" alt=\"SVD illustration\" align=\"center\"  style=\"width: 500px;\"/>\n",
    "\n",
    "**SVD**\n",
    "- can be done for matrices of any shape\n",
    "- is almost unique\n",
    "    - up to making the same permutation of the columns of $U$, elements of $\\Sigma$, and columns of $V$\n",
    "    - up to forming linear combinations of any columns of $U$ and $V$ whose corresponding elements of $\\sigma_i$ happen to be equal\n",
    "- is very stable numerically\n",
    "- allows us to easily pick a \"representative\" solution (the one with the smallest length) when there are infinitely many<br>=> needed for data analysis!\n",
    "- allows us to find an \"almost\" solution when there are none<br>=> needed for data analysis!\n",
    "- we will not discuss the algorithm, but it is part of all major libraries\n",
    "\n",
    "**Matrix inversion (of a square matrix) with SVD**:\n",
    "- for square matrices, $U$ is also square; $U$, $\\Sigma$, $V$ have the same size\n",
    "- inverse becomes easy:\n",
    "\n",
    "$$\n",
    "A = U\\cdot[{\\rm diag}(\\sigma_i)]\\cdot V^T \\ \\ \\Rightarrow\\ \\ A^{-1}=V\\cdot[{\\rm diag}(1/\\sigma_i)]\\cdot U^T\n",
    "$$\n",
    "\n",
    "- solution of linear equations becomes easy:\n",
    "\n",
    "$$A\\mathbf{x}=\\mathbf{b}\\Rightarrow \\mathbf{x}=V\\cdot[{\\rm diag}(1/\\sigma_i)]\\cdot (U^T\\mathbf{b})$$\n",
    "\n",
    "- problems if some of the $\\sigma_i$ are zero (or close to zero...)\n",
    "- the *condition number* (see previous lecture) is defined as: max$(|\\sigma_i|)/$min$(|\\sigma_i|)$\n",
    "     - condition number infinite => matrix singular\n",
    "     - condition number too large ($\\gtrsim 1/\\varepsilon_m$) => matrix ill-conditioned\n",
    "     - in these cases we set (somewhat paradoxically) $1/\\sigma_i=0$ (remove singular values)\n",
    "\n",
    "A nonsingular matrix $A$ maps a vector space into one of the same dimension:\n",
    "\n",
    "![non-singular matrices](images/NRSVD_nonsing.png)\n",
    "\n",
    "*Image credit*: [\"Numerical Recipes\"](http://numerical.recipes/oldverswitcher.html) book, which has a very nice chapter on SVD\n",
    "\n",
    "**Singular matrices**:\n",
    "- if $A$ is singular, then there is a subspace of $\\mathbf{x}$ (it's called **nullspace**) so that $A\\mathbf{x}=0$\n",
    "     - for an example singular matrix $A$ in 3D, let's say that $A\\mathbf{x}=0$, where $\\mathbf{x}=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$\n",
    "     - this means automatically that $A\\mathbf{x}=0$ also for all $\\mathbf{x}=\\begin{pmatrix}x\\\\0\\\\0\\end{pmatrix}$, where $x$ can be any real number\n",
    "     - in words, the whole $x$-axis gets mapped onto 0 by this particular singular matrix $A$\n",
    "     - let's also assume that no other vectors get mapped to 0\n",
    "     - in this case, the $x$-axis would be the nullspace of $A$\n",
    "     - if we also had $A\\mathbf{x}=0$ for, let's say, all $\\mathbf{x}=\\begin{pmatrix}0\\\\y\\\\0\\end{pmatrix}$, then the nullspace of $A$ would be the $xy$-plane\n",
    "- there is also some subspace of $\\mathbf{b}$ (it's called **range of $A$**, or **image of $A$**) that can be reached by $A$: i.e. there exists $\\mathbf{x}$ such that $A\\mathbf{x}=\\mathbf{b}$\n",
    "- the dimension of the range is called the **rank** of $A$ (for non-singular matrices the rank is $N$, because we can reach any vector)\n",
    "- for singular matrices the rank is smaller than $N$ and the nullspace has dimension greater than zero\n",
    "- the sum of the dimensions of the nullspace and range is always $N$, which means that the more singular a matrix is (the bigger its nullspace) the smaller its range becomes\n",
    "\n",
    "> SVD explicitly constructs orthonormal bases for the *nullspace* and *range* of the matrix\n",
    "\n",
    "- columns of $U$ belonging to $\\sigma_i\\neq0$ span the range (orthonormal basis)\n",
    "- columns of $V$ belonging to $\\sigma_i=0$ span the nullspace (orthonormal basis)\n",
    "- solutions of $A\\mathbf{x}=0$ are defined by the nullspace basis => read out from $V$\n",
    "- solutions of $A\\mathbf{x}=\\mathbf{b}\\neq0$:\n",
    "    - if $\\mathbf{b}$ is not in the range of $A$ => no solution\n",
    "    - if $\\mathbf{b}$ is in the range of $A$ => solution exists (and any linear combination of nullspace vectors can be added to it)\n",
    "    \n",
    "A singular matrix $A$ maps a vector space into one of lower dimensionality (the range of $A$): \n",
    "    \n",
    "![SVD of singular matrices](images/NRSVD_sing.png)\n",
    "    \n",
    "- the nullspace of $A$ is mapped to zero\n",
    "- the solutions of $A\\cdot\\mathbf{x} = \\mathbf{d}$ consist of any one particular solution plus any vector in the nullspace\n",
    "- SVD selects the particular solution closest to zero\n",
    "- the point $\\mathbf{c}$ lies outside of the range of $A$ ($A\\cdot\\mathbf{x} = \\mathbf{c}$ has no solution)\n",
    "- SVD finds the best \"compromise solution\", namely a solution of $A\\cdot\\mathbf{x} = \\mathbf{c}'$ => see \"linear least-squares\" later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.15065941e+00+0.83219225j,   7.13857912e-01+0.65063458j,\n",
       "          2.83271868e+00-0.0997891j ,   4.73147342e-01-0.83331881j,\n",
       "         -6.17771812e-02-0.70101645j],\n",
       "       [ -4.84956847e-01-0.37133043j,   7.61437548e-01+0.34330612j,\n",
       "         -8.22760381e-03-0.71509216j,   5.32001801e-01+1.66660794j,\n",
       "          3.25920127e-01-0.10088881j],\n",
       "       [ -4.75438505e-01-1.9032805j ,   6.48972402e-04-1.81793839j,\n",
       "         -4.08067452e-01-0.83213514j,   5.89603383e-01-0.40209971j,\n",
       "         -1.18218017e+00+0.1793514j ],\n",
       "       [  7.88949616e-01+1.65663806j,  -7.23988533e-01-1.47658886j,\n",
       "         -1.72668656e-01-0.40953176j,   1.14773387e+00-1.06719383j,\n",
       "         -1.54814693e+00-1.11999095j],\n",
       "       [  2.13617367e-01+1.01970608j,   6.13151034e-02+0.96819329j,\n",
       "          7.97078560e-02+0.15177979j,  -2.32039578e-01-0.93876035j,\n",
       "          6.94589248e-01-1.23592639j],\n",
       "       [ -2.00208338e-01+1.56380731j,   1.44891799e+00+1.16846287j,\n",
       "         -4.56821708e-01-0.75286773j,  -9.98629263e-01-0.95857915j,\n",
       "          4.50097260e-01-0.24685933j],\n",
       "       [  1.16322686e+00+0.5609819j ,   9.27513370e-02+0.64458597j,\n",
       "          2.15025512e+00-0.39227101j,  -1.70569236e+00+0.62738785j,\n",
       "         -8.95335846e-02-0.15406798j],\n",
       "       [ -1.00274015e+00-0.56723208j,   1.67010972e-01-0.0350059j ,\n",
       "          1.36659566e-01-0.36608524j,  -5.55499190e-02-1.29707688j,\n",
       "         -5.90910981e-01+0.30430717j],\n",
       "       [ -1.99263948e-01+0.64560999j,  -5.42620626e-02-1.010249j  ,\n",
       "          9.39738411e-01+0.96142758j,   1.52170258e+00-0.79169068j,\n",
       "         -1.27889525e+00+1.73250465j]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Singular Value Decomposition\n",
    "m, n = 9, 5\n",
    "a = scipy.random.randn(m, n) + 1.j*scipy.random.randn(m, n)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.06083124+0.32711167j,  0.32037176-0.39931357j,\n",
       "          0.00474184-0.45878096j,  0.12958971+0.07811453j,\n",
       "         -0.10160740-0.04350875j, -0.18284833-0.3785505j ,\n",
       "         -0.06273821-0.20578875j, -0.33758759-0.04844874j,\n",
       "         -0.18855368-0.08190841j],\n",
       "        [ 0.19924708-0.12695631j,  0.27682056+0.10679674j,\n",
       "         -0.17943060-0.11213056j,  0.00581658-0.17378163j,\n",
       "          0.04682895+0.24140232j, -0.24718003-0.21037083j,\n",
       "          0.14244737+0.71098225j, -0.04828336-0.22903119j,\n",
       "          0.12361940-0.1395271j ],\n",
       "        [-0.07247349-0.43932048j, -0.18048040-0.0662489j ,\n",
       "         -0.20243828-0.2553385j ,  0.20772180+0.24897389j,\n",
       "         -0.19216882-0.40458216j,  0.27459991+0.03873682j,\n",
       "          0.06870517+0.20461515j, -0.32572868-0.17062807j,\n",
       "          0.05995322+0.30091722j],\n",
       "        [-0.07101630-0.13768162j, -0.25097224-0.60629793j,\n",
       "          0.10510083+0.20386007j, -0.29719480-0.03188879j,\n",
       "         -0.35872151+0.18866779j,  0.23841348-0.18824209j,\n",
       "         -0.03976657+0.13541322j,  0.16202744-0.22338605j,\n",
       "         -0.04390026-0.22117221j],\n",
       "        [ 0.03133493+0.28338166j,  0.09330573-0.14047424j,\n",
       "          0.03680505+0.24525992j, -0.24154547+0.30243761j,\n",
       "         -0.02631471-0.02297587j, -0.22768931-0.08349245j,\n",
       "          0.07735898-0.01581643j, -0.01240679-0.1981498j ,\n",
       "          0.54542692+0.52514578j],\n",
       "        [ 0.16676872+0.34103252j,  0.01775759-0.15484913j,\n",
       "         -0.25952200+0.31461395j,  0.28720048+0.16424659j,\n",
       "          0.43070528+0.08943047j,  0.54977820-0.14384433j,\n",
       "          0.05907353+0.13459403j, -0.11184210+0.0122172j ,\n",
       "         -0.06824272-0.02060225j],\n",
       "        [ 0.29792348+0.35789158j, -0.11640724+0.00277521j,\n",
       "          0.24755124-0.35080254j,  0.05223565-0.20274956j,\n",
       "         -0.28931039+0.07871968j,  0.26111452+0.1916199j ,\n",
       "          0.51785989+0.09698772j,  0.14312131+0.08291687j,\n",
       "         -0.01163851+0.2038155j ],\n",
       "        [-0.16195602+0.0017096j , -0.06171105-0.08913529j,\n",
       "         -0.20652288-0.1417546j ,  0.33217062+0.34951634j,\n",
       "          0.03947648-0.13197067j, -0.19401680-0.13741636j,\n",
       "          0.21986565+0.00144382j,  0.72864850-0.02731866j,\n",
       "         -0.00106294-0.10606326j],\n",
       "        [-0.35731280-0.12277562j, -0.10688320-0.30473074j,\n",
       "          0.19277311-0.27175836j,  0.18537731-0.4271057j ,\n",
       "          0.48519254+0.15301539j,  0.05682925+0.04811905j,\n",
       "         -0.08622229+0.0148137j ,  0.07751518-0.02039079j,\n",
       "          0.32498825+0.20274307j]]),\n",
       " array([ 5.49014771,  4.7074565 ,  3.80064978,  2.61794967,  2.12073593]),\n",
       " array([[ 0.43268241+0.j        ,  0.47756561+0.04761948j,\n",
       "          0.01533596-0.40451405j, -0.30923219+0.45750824j,\n",
       "          0.00806713-0.3370151j ],\n",
       "        [-0.51646584+0.j        ,  0.30283432+0.24307198j,\n",
       "          0.16210507+0.25973025j,  0.36700948+0.35371744j,\n",
       "          0.32318381-0.35280704j],\n",
       "        [ 0.45018100+0.j        , -0.03172837-0.00263711j,\n",
       "          0.22881204+0.78959636j, -0.06080517+0.13753574j,\n",
       "         -0.26363061-0.16849814j],\n",
       "        [-0.50542293+0.j        ,  0.42886364-0.06950005j,\n",
       "         -0.03796320+0.16382726j, -0.50230702+0.01717492j,\n",
       "         -0.47429399+0.22349653j],\n",
       "        [ 0.29653618+0.j        ,  0.60974135+0.23941267j,\n",
       "         -0.15211694+0.12311717j,  0.32658085-0.23102804j,\n",
       "          0.14293475+0.51401032j]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.svd(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 9), (5,), (5, 5))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, s, Vh = LA.svd(a)\n",
    "U.shape,  s.shape, Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.15065941e+00+0.83219225j,   7.13857912e-01+0.65063458j,\n",
       "          2.83271868e+00-0.0997891j ,   4.73147342e-01-0.83331881j,\n",
       "         -6.17771812e-02-0.70101645j],\n",
       "       [ -4.84956847e-01-0.37133043j,   7.61437548e-01+0.34330612j,\n",
       "         -8.22760381e-03-0.71509216j,   5.32001801e-01+1.66660794j,\n",
       "          3.25920127e-01-0.10088881j],\n",
       "       [ -4.75438505e-01-1.9032805j ,   6.48972402e-04-1.81793839j,\n",
       "         -4.08067452e-01-0.83213514j,   5.89603383e-01-0.40209971j,\n",
       "         -1.18218017e+00+0.1793514j ],\n",
       "       [  7.88949616e-01+1.65663806j,  -7.23988533e-01-1.47658886j,\n",
       "         -1.72668656e-01-0.40953176j,   1.14773387e+00-1.06719383j,\n",
       "         -1.54814693e+00-1.11999095j],\n",
       "       [  2.13617367e-01+1.01970608j,   6.13151034e-02+0.96819329j,\n",
       "          7.97078560e-02+0.15177979j,  -2.32039578e-01-0.93876035j,\n",
       "          6.94589248e-01-1.23592639j],\n",
       "       [ -2.00208338e-01+1.56380731j,   1.44891799e+00+1.16846287j,\n",
       "         -4.56821708e-01-0.75286773j,  -9.98629263e-01-0.95857915j,\n",
       "          4.50097260e-01-0.24685933j],\n",
       "       [  1.16322686e+00+0.5609819j ,   9.27513370e-02+0.64458597j,\n",
       "          2.15025512e+00-0.39227101j,  -1.70569236e+00+0.62738785j,\n",
       "         -8.95335846e-02-0.15406798j],\n",
       "       [ -1.00274015e+00-0.56723208j,   1.67010972e-01-0.0350059j ,\n",
       "          1.36659566e-01-0.36608524j,  -5.55499190e-02-1.29707688j,\n",
       "         -5.90910981e-01+0.30430717j],\n",
       "       [ -1.99263948e-01+0.64560999j,  -5.42620626e-02-1.010249j  ,\n",
       "          9.39738411e-01+0.96142758j,   1.52170258e+00-0.79169068j,\n",
       "         -1.27889525e+00+1.73250465j]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstruct the original matrix from the decomposition:\n",
    "sigma = scipy.zeros((m, n))\n",
    "for i in range(min(m, n)):\n",
    "    sigma[i, i] = s[i]\n",
    "a1 = scipy.dot(U, scipy.dot(sigma, Vh))\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.allclose(a, a1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
