{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS 325 Scientific Computing -- Fall 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Numerical methods\n",
    "\n",
    "*Acknowledgements:* My lecture notes for this chapter draw from the \"Numerical Mathematics for Physicists\" course taught by [Martin Kerscher](https://homepages.physik.uni-muenchen.de/~Martin.Kerscher/), as well as lectures by [Lode Pollet](https://www.theorie.physik.uni-muenchen.de/lsschollwoeck/members/professors/pollet/) and the \"Numerical Recipes\" books.\n",
    "\n",
    "## 2.1 Linear algebra\n",
    "\n",
    "System of linear equations:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}\n",
    "  a_{11} x_1 + a_{12} x_2 + \\cdots + a_{1N} x_N & = b_1 \\\\\n",
    "  \\vdots & \\vdots \\\\\n",
    "  a_{M1} x_1 + a_{M2} x_2 + \\cdots + a_{MN} x_N & = b_M\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$M$ equations f√ºr $N$ unknowns\n",
    "\n",
    "- numerics: typically thousands of variables\n",
    "- for linear problems or linear approximations to other problems\n",
    "- needed for many numerical setups (for example splines etc.)\n",
    "- many highly optimized libraries available\n",
    "\n",
    "Matrix notation:\n",
    "\n",
    "$$\n",
    "A  \\mathbf{x} = \\mathbf{b} \n",
    "$$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{M \\times N}; \\quad \\mathbf{x} \\in \\mathbb{R}^N; \\quad \\mathbf{b}  \\in \\mathbb{R}^M $:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "  a_{11} & \\cdots & a_{1N} \\\\ \n",
    "  \\vdots &  & \\vdots \\\\ \n",
    "  a_{M1} & \\cdots & a_{MN}\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "  x_{1} \\\\ \n",
    "  \\vdots  \\\\ \n",
    "  x_{N} \n",
    "\\end{pmatrix} \n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  b_{1} \\\\ \n",
    "  \\vdots  \\\\ \n",
    "  b_{M} \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "How to solve a system of linear equations with Gaussian elimination (reminder):\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "  1 & 5 & 7 \\\\\n",
    "  3 & 0 & 4 \\\\\n",
    "  7 & 5 & 5 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  x_1\\\\ x_2\\\\ x_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  1\\\\ 2\\\\ 3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "write as\n",
    "\n",
    "$$\n",
    "\\Big( A^{(0)}|\\mathbf{b}^{(0)}\\Big)= \n",
    "\\left(\n",
    "  \\begin{array}{ccc|c}\n",
    "    1 & 5 & 7  & 1\\\\\n",
    "    3 & 0 & 4  & 2\\\\\n",
    "    7 & 5 & 5  & 3\n",
    "  \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "We are allowed to:\n",
    "- swap rows\n",
    "- multiply a row by a (non-zero) constant\n",
    "- add a multiple of one row to another row\n",
    "\n",
    "Transform column by column until we arrive at upper triangular matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\mathbf{II}^{(1)} &= \\mathbf{II}^{(0)}-\\tfrac{3}{1}\\ \\mathbf{I}^{(0)}\\\\\n",
    "  \\mathbf{III}^{(1)} &= \\mathbf{III}^{(0)}-\\tfrac{7}{1}\\ \\mathbf{I}^{(0)}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "implies\n",
    "\n",
    "$$\n",
    "\\Big(A^{(1)}|\\mathbf{b}^{(1)}\\Big)= \n",
    "\\left(\n",
    "  \\begin{array}{ccc|c}\n",
    "    1 & 5   & 7    & \\ \\ 1\\\\\n",
    "    0 & -15 & -17  & -1\\\\\n",
    "    0 & -30 & -44  & -4\n",
    "  \\end{array}\n",
    "\\right). \n",
    "$$\n",
    "\n",
    "Next step:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\mathbf{III}^{(2)} &= \\mathbf{III}^{(1)}-\\tfrac{30}{15}\\ \\mathbf{II}^{(1)}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "implies\n",
    "\n",
    "$$\n",
    "\\Big(A^{(2)}|\\mathbf{b}^{(2)}\\Big)= \n",
    "\\left(\n",
    "  \\begin{array}{ccc|c}\n",
    "    1 & 5 & 7      & \\ \\ 1\\\\\n",
    "    0 & -15 & -17  & -1\\\\\n",
    "    0 &   0 & -10  & -2\n",
    "  \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "This is equivalent to\n",
    "\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "x_1 & + 5\\ x_2  & +7\\ x_3  & = \\ \\ 1 \\\\\n",
    "    & - 15\\ x_2 & -17\\ x_3 & =-1 \\\\\n",
    "    &          & -10\\ x_3 & =-2\n",
    "\\end{array} .\n",
    "$$\n",
    "\n",
    "Now it is easy to solve for $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "x_3 = \\tfrac{1}{5}, \\\n",
    "x_2 = -\\tfrac{4}{25}, \\ \n",
    "x_1 = \\tfrac{2}{5} .\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<tr>\n",
    "<td><img src=\"images/infmanysolutions.png\" alt=\"Infinitely many solutions\" align=\"right\"  style=\"width: 200px;\"/></td>\n",
    "<td><img src=\"images/uniquesolution.png\" alt=\"Infinitely many solutions\" align=\"right\"  style=\"width: 200px;\"/></td>\n",
    "<td><img src=\"images/nosolution.png\" alt=\"Unique solution\" align=\"right\" style=\"width: 200px;\"/></td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "Classification\n",
    "- no solution\n",
    "- unique solution (rank($A$)=$N$)\n",
    "- infinitely many solutions\n",
    "\n",
    "<br><br><br><br><br><br>\n",
    "For quadratic matrices:\n",
    "\n",
    "<center>$A$ is invertible</center>\n",
    "$$\\Leftrightarrow$$\n",
    "<center>rank($A$)=$N$ </center>\n",
    "$$\\Leftrightarrow$$\n",
    "<center>det$(A)\\neq0$</center>\n",
    "$$\\Leftrightarrow$$\n",
    "<center>$A\\mathbf{x}=0$ has only the solution $\\mathbf{x}=0$ </center>\n",
    "\n",
    "<br>\n",
    "Formally\n",
    "\n",
    "$$A^{-1}\\mathbf{b}=\\mathbf{x}$$\n",
    "\n",
    "so solving a system of linear equations is related to inverting a matrix!\n",
    "\n",
    "> never use a direct implementation of matrix inversion => slow and numerically unstable\n",
    "\n",
    "(but you can do the opposite: solve $A\\mathbf{x}_j=\\mathbf{e}_j$ for each unit vector $\\mathbf{e}_j$ to obtain the inverse)\n",
    "\n",
    "Tasks of Computational Linear Algebra:\n",
    "- solving sets of linear equations\n",
    "- matrix determinants\n",
    "- matrix inversion\n",
    "- singular value decomposition of a matrix\n",
    "- linear least squares => see section on data fitting\n",
    "\n",
    "Numerical problems:\n",
    "- equations may formally have a unique solution, but some of the equations may be close to being linearly dependent<br> => roundoff errors in the solution process can make them linearly dependent <br>=> failure of the solution procedure\n",
    "- large $N$: roundoff errors may swamp the true solution <br>=> numerical instability <br>=> wrong solution (need to check!)\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "  \\begin{pmatrix}\n",
    "    10 & 7 & 8 & 7 \\\\\n",
    "    7 & 5 & 6 & 5 \\\\\n",
    "    8 & 6 & 10 & 9 \\\\\n",
    "    7 & 5 & 9 & 10\\\\\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    x_1\\\\ x_2\\\\ x_3\\\\ x_4\n",
    "  \\end{pmatrix} = \n",
    "  \\begin{pmatrix}\n",
    "    32\\\\ 23\\\\ 33\\\\ 31\n",
    "  \\end{pmatrix}\\Rightarrow\n",
    "  \\mathbf{x}=  \\begin{pmatrix}\n",
    "    1\\\\ 1\\\\ 1\\\\ 1\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Very similar system:\n",
    "\n",
    "$$\n",
    "  \\begin{pmatrix}\n",
    "    10 & 7 & 8 & 7 \\\\\n",
    "    7 & 5 & 6 & 5 \\\\\n",
    "    8 & 6 & 10 & 9 \\\\\n",
    "    7 & 5 & 9 & 10\\\\\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    x_1\\\\ x_2\\\\ x_3\\\\ x_4\n",
    "  \\end{pmatrix} = \n",
    "  \\begin{pmatrix}\n",
    "    32.1\\\\ 22.9\\\\ 33.1\\\\ 30.9\n",
    "  \\end{pmatrix}\\Rightarrow\n",
    "  \\mathbf{x}=  \\begin{pmatrix}\n",
    "    9.2\\\\ -12.6\\\\ 4.5\\\\ -1.1\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- a relative error of $0.1/23 \\approx 0.00434$ in $\\mathbf{b}$ results in a relative error of $12.6/1$ on the result\n",
    "- relative error enhancement factor $\\approx3000$.\n",
    "- general problem in systems of linear equations, independent of numerical method\n",
    "\n",
    "This problem can be quantified: **condition number**\n",
    "\n",
    "$$\\kappa (A) = \\Vert A^{-1} \\Vert \\Vert A \\Vert$$\n",
    "\n",
    "Here $\\Vert A \\Vert$ is a **matrix norm**. There are many matrix norms:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\Vert A \\Vert _{\\rm rows} &:= \\max_i \\left( \\sum_k \\vert a_{ik} \\vert \\right)\\\\  \n",
    "\\Vert A \\Vert _{\\rm cols} &:= \\max_k \\left( \\sum_i \\vert a_{ik} \\vert \\right) \\\\  \n",
    "\\Vert A \\Vert _{\\rm Frobenius} &:= \\left( \\sum_i \\sum_k a_{ik}^2 \\right) ^{\\frac{1}{2}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "resulting in different condition numbers => order of magnitude matters\n",
    "\n",
    "Relative error on the result:\n",
    "\n",
    "$$\\frac{\\Vert \\delta_\\mathbf{x} \\Vert}{\\Vert \\mathbf{x} \\Vert} \\lesssim \\kappa(A) \\left( \\frac{\\Vert \\Delta_A \\Vert}{\\Vert A \\Vert} \n",
    "+ \\frac{\\Vert \\Delta_\\mathbf{b} \\Vert}{\\Vert \\mathbf{b} \\Vert} \\right)$$\n",
    "\n",
    "Using the [NumPy Linear Algebra submodule](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.linalg.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  7,  8,  7],\n",
       "       [ 7,  5,  6,  5],\n",
       "       [ 8,  6, 10,  9],\n",
       "       [ 7,  5,  9, 10]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([10, 7, 8, 7, 7, 5, 6, 5, 8, 6, 10, 9, 7, 5, 9, 10])\n",
    "b = a.reshape((4, 4))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.54504869860253"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "LA.norm(b)          # Frobenius norm by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3009.578708058694"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.norm(b)*LA.norm(LA.inv(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3009.578708058694"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.cond(b,'fro')    # condition number directly, specifying Frobenius norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian elimination step by step:\n",
    "\n",
    "$\\ $ | $\\ $  | $\\ $ | $\\ $ | $\\ $\n",
    "---|---|----|----|----\n",
    "![start](images/Kap4Gauss0.png) | $\\longrightarrow$ | ![step 1](images/Kap4Gauss1.png) | $\\longrightarrow$ |![step 2](images/Kap4Gauss2.png) \n",
    "\n",
    "Triangular form:\n",
    "\n",
    "![triangular](images/Kap4GaussN.png)\n",
    "\n",
    "Step $(n-1)\\rightarrow(n)$ explicitly:\n",
    "- the first $n$ rows are unchanged:\n",
    "\n",
    "  $$\n",
    "  a_{ij}^{(n)} = a_{ij}^{(n-1)},\\ b_i^{(n)} = b_i^{(n-1)}, \n",
    "  \\quad \\text{for } i=1,\\ldots,n, \\ j=1,\\ldots,N\n",
    "  $$\n",
    "- the first $n$ columns are unchanged, too\n",
    "- for the rest:\n",
    "\n",
    "  $$\n",
    "  \\begin{align*} \n",
    "    a_{in}^{(n)} & = 0, \\\\\n",
    "    a_{ik}^{(n)} & = a_{ik}^{(n-1)} - l_{in} a_{nk}^{(n-1)}, \\\n",
    "    \\qquad \\text{for } i,k = n + 1 , \\ldots , N \\\\\n",
    "    b_i^{(n)} & = b_i^{(n-1)} - l_{in} b_n^{(n-1)}.\n",
    "  \\end{align*}\n",
    "  $$\n",
    "  where $$l_{in} = \\frac{a_{in}^{(n-1)}}{a_{nn}^{(n-1)}}$$ and under the condition $a_{nn}^{(n-1)}\\neq0$.\n",
    "\n",
    "> careful not to divide by zero or a \"small\" number!\n",
    "\n",
    "Complexity of Guassian elimination:\n",
    "\n",
    "- step $n$ of the process\n",
    "     - $N-n$ divisions for $l_{in}$\n",
    "     - $2(N-n)^2$ additions and multiplications for $a_{ik}^{(n)}$\n",
    "     - $2(N-n)$ additions and multiplications for $b_i^{(n)}$\n",
    "     \n",
    "  Altogether ($i=N-n$):\n",
    "  \n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\#\\text{FLOPs} & = 3 \\sum_{n = 1}^{N-1} (N - n) + 2 \\sum_{n = 1}^{N-1}(N - n)^2 \\\\\n",
    "  & = 3 \\left[ (N-1)N - \\sum_{n = 1}^{N-1} n \\right] + 2 \\sum_{i = N-1}^{1}i^2 \\\\\n",
    "  & = 3 \\left[ (N-1)N - \\tfrac{1}{2} (N-1)N \\right] + \\tfrac{2}{6}(N-1)N(2(N-1)+1)\\\\\n",
    "  & = \\frac{2}{3} N^3 - \\frac{1}{6} N^2 -\\frac{1}{2} N \n",
    "  \\end{align*}\n",
    "  $$\n",
    "- solving for each $x_i$ in the end\n",
    "    - $2(N-(i+1))$ additions and multiplications in the sum\n",
    "    - one multiplication and one addition outside the sum\n",
    "    - one multiplication for $x_N$\n",
    "    \n",
    "  Altogether:\n",
    "  \n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\#\\text{FLOPs} & =  1 + \\sum_{i = 1}^{N-1} ( 2(N-i-1)\\ +\\ 2) \\\\\n",
    "  & = 1 + 2 \\sum_{i = 1}^{N-1}  (N - i)\\ = 1 + 2N(N-1) - \\frac{2}{2} N(N-1) = \\\\\n",
    "  & =  N^2 -N + 1\n",
    "  \\end{align*}\n",
    "  $$\n",
    "- memory and element access is also very important for matrix operations<br>\n",
    "  => beyond the scope of this lecture\n",
    "\n",
    "In total:\n",
    "\n",
    "$$\n",
    "\\#\\text{FLOPs} = \\frac{2}{3} N^3 + \\frac{5}{6} N^2 - \\frac{3}{2} N + 1 =\\mathcal{O}(N^3)\n",
    "$$\n",
    "\n",
    "### LU decomposition\n",
    "\n",
    "$$\n",
    "  A = L U \n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    1 &   &  &  0 \\\\ \n",
    "    l_{21} & 1 &  &  \\\\ \n",
    "    \\vdots &  \\ddots & \\ddots &  \\\\ \n",
    "    l_{N1} & \\cdots & l_{NN-1} & 1\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    u_{11} & u_{12} & \\cdots & u_{1N} \\\\ \n",
    "    & u_{22} &  & \\vdots \\\\ \n",
    "    &  & \\ddots & \\vdots \\\\ \n",
    "    0&  &  & u_{NN}\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Solving a set of linear equations:\n",
    "- first solve $L \\mathbf{y} = \\mathbf{b}$ for $\\mathbf{y}=U \\mathbf{x}$ \n",
    "- then solve $U \\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$ as before\n",
    "- if we *already have* the LU decomposition we need $2(N^2-N+1)$ FLOPs to solve the set of equations for *any* $\\mathbf{b}$, so $\\mathcal{O}(N^2)$\n",
    "- we get the determinant (almost) for free:\n",
    "\n",
    "  $$\\det(A) = \\det(LU) = \\det(L)\\det(U) = \\prod_{i=1}^{N}u_{ii}$$\n",
    "\n",
    "How to compute the LU decomposition:\n",
    "- same steps as Gaussian elimination for $U$\n",
    "- we have already computed the elements of $L$ along the way\n",
    "\n",
    "$$\n",
    "L = \\begin{pmatrix}\n",
    "  1 &  &  &  \\\\\n",
    "  l_{21} & \\ddots & \\emptyset &  \\\\\n",
    "  \\vdots & \\ddots & \\ddots &  \\\\\n",
    "  l_{N1} & \\cdots & l_{NN-1} & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Pivoting\n",
    "\n",
    "- so far we have always assumed $a_{nn}^{(n-1)}\\neq0$:\n",
    "\n",
    "  $$\n",
    "  a_{ik}^{(n)} = a_{ik}^{(n-1)} - l_{in} a_{nk}^{(n-1)}  \\text{ with } \\ \n",
    "  l_{in} = \\frac{a_{in}^{(n-1)}}{a_{nn}^{(n-1)}} .\n",
    "  $$\n",
    "- if this is not the case => swap rows!\n",
    "- swapping rows can be described by a permutation matrix $P$, for example:\n",
    "\n",
    "  $$\n",
    "  \\begin{pmatrix}\n",
    "    0&1&0\\\\\n",
    "    0&0&1\\\\\n",
    "    1&0&0\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}x_2\\\\x_3\\\\x_1\\end{pmatrix}\n",
    "  $$\n",
    "- then we need to solve $PA\\mathbf{x}=P\\mathbf{b}$ and decompose $PA=LU$\n",
    "\n",
    "The matrix element $a_{nn}^{(n-1)}$ is called **pivot element**\n",
    "- even though we can choose $a_{nn}^{(n-1)} \\neq 0$ it can happen that $0<\\vert a_{nn}^{(n-1)} \\vert \\ll 1$ <br>=> numerical problems\n",
    "- subtractions in every step, so we can lose significant digits, e.g. in\n",
    "\n",
    "  $$\n",
    "  a_{ik}^{(1)}=a_{ik}^{(0)}-l_{i1} a_{1k}^{(0)}\n",
    "  $$\n",
    "- for example, if we have $|a_{nn}^{(1)}|\\ll1$ then $l_{in}$ is very large and the roundoff error from before is amplified in\n",
    "\n",
    "  $$\n",
    "  a_{ik}^{(2)} = a_{ik}^{(1)} -\n",
    "  \\overset{\\text{roundoff error}}{\\underset{\\text{very large}}\n",
    "    {\\underset{\\uparrow}{l_{i2}\\ } \\overset{\\downarrow}{a_{2k}^{(1)}}}}\n",
    "  $$\n",
    "\n",
    "=> pivoting is **essential** for numerical stability!\n",
    "\n",
    "**Column maximization strategy**\n",
    "- in every step of Gaussian elimination swap the remaining rows until\n",
    "\n",
    "  $$\n",
    "  \\vert a_{nn}^{(n-1)} \\vert = \\max_{i \\geq n} \\vert a_{in}^{(n-1)} \\vert \n",
    "  $$\n",
    "- further improvements on column maximization possible\n",
    "- book-keeping over raw swaps in terms of the permutation matrix\n",
    "\n",
    "Extreme example:\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "  \\epsilon & 1 \\\\\n",
    "  1        & 1 \n",
    "\\end{pmatrix}\n",
    "= L U =\n",
    "\\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  \\epsilon^{-1} & 1 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  \\epsilon & 1 \\\\\n",
    "   0 & 1 - \\epsilon^{-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "After pivoting:\n",
    "\n",
    "$$\n",
    "PA = A' = \n",
    "\\begin{pmatrix}\n",
    "  1        & 1 \\\\\n",
    "  \\epsilon & 1 \n",
    "\\end{pmatrix}\n",
    "= L' U' =\n",
    "\\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  \\epsilon & 1 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "   1 & 1 \\\\\n",
    "   0 & 1 - \\epsilon\n",
    "\\end{pmatrix} .\n",
    "$$\n",
    "\n",
    "Assume $\\epsilon<\\epsilon_m$ (machine precision), then\n",
    "\n",
    "$$1-\\epsilon^{-1}\\rightarrow-\\epsilon^{-1}$$\n",
    "and the component $u_{22}$ of the matrix $U$ is not exact. Instead of $LU$ we get\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  \\epsilon^{-1} & 1 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "   1 & 1 \\\\\n",
    "   0 & -\\epsilon^{-1}\n",
    "\\end{pmatrix} \n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  \\epsilon & 1 \\\\\n",
    "  1 & 0 \n",
    "\\end{pmatrix}\n",
    "\\ne A.\n",
    "$$\n",
    "\n",
    "**Special cases**:\n",
    "- band diagonal matrices (especially tri-diagonal => spline interpolation)\n",
    "    - only need to save diagonals (memory saving)\n",
    "    - not solved with LU decomposition, but for each $\\mathbf{b}$ with Gaussian elimination ($\\mathcal{O}(N)$) for this special case\n",
    "    - usually no pivoting necessary\n",
    "- symmetric matrices => Cholesky decomposition\n",
    "    - similar to LU decomposition\n",
    "    - no pivoting necessary\n",
    "    - also some memory saving\n",
    "- existence and uniqueness of LU decomposition\n",
    "    - LU decompositions are (in general) not unique\n",
    "    - LU decompositions do not always exist\n",
    "    - a square matrix always has an LU decomposition **with pivoting** (LUP decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix inversion\n",
    "\n",
    "> essentially the same as solving sets of linear equations\n",
    "\n",
    "Assume $\\det(A) \\neq 0$ and we have a way to solve a system of linear equations $A \\mathbf{x} =\\mathbf{b}$ (e.g. LU decomposition)\n",
    "\n",
    "To obtain $A^{-1}$:\n",
    "- solve the linear equation $A \\mathbf{x}_i = \\mathbf{e}_i$ for all unit vectors $\\mathbf{e}_i$ and obtain the $\\mathbf{x}_i$\n",
    "- then\n",
    "\n",
    "  $$\n",
    "  \\mathbf{x}_i = A^{-1} \\mathbf{e}_i = \n",
    "  \\begin{pmatrix}\n",
    "    (a^{-1})_{11} & \\cdots & (a^{-1})_{1N} \\\\\n",
    "    &&\\\\ \n",
    "    \\vdots &  & \\vdots \\\\ \n",
    "    &&\\\\\n",
    "    (a^{-1})_{N1} & \\cdots & (a^{-1})_{NN}\n",
    "  \\end{pmatrix}  \n",
    "  \\begin{pmatrix}\n",
    "    0 \\\\ \n",
    "    \\vdots \\\\ \n",
    "    1 \\\\ \n",
    "    \\vdots \\\\ \n",
    "    0\n",
    "  \\end{pmatrix} =\n",
    "  \\begin{pmatrix}\n",
    "    (a^{-1})_{1i} \\\\ \n",
    "    \\\\ \n",
    "    \\vdots \\\\ \n",
    "    \\\\ \n",
    "    (a^{-1})_{Ni}\n",
    "  \\end{pmatrix}  \n",
    "  $$\n",
    "  \n",
    "  meaning that the $\\mathbf{x}_i$ are the column vectors of $A^{-1}$:\n",
    "  \n",
    "  $$\n",
    "  A^{-1} = ( \\mathbf{x}_1 , \\mathbf{x}_2 , \\ldots , \\mathbf{x}_N ) .\n",
    "  $$\n",
    "\n",
    "### Iterative improvement\n",
    "\n",
    "- let $\\mathbf{x}$ be the exact solution of $A \\mathbf{x} = \\mathbf{b}$ (which we don't know)\n",
    "- let $\\hat{\\mathbf{x}} = \\mathbf{x} +\\delta \\mathbf{x}$ be the numerical (non-exact) solution  \n",
    "\n",
    "Then\n",
    "\n",
    "$$ \n",
    "A \\hat{\\mathbf{x}} = A \\mathbf{x} + A \\delta \\mathbf{x} = \\mathbf{b} + \\mathbf{r} \n",
    "$$\n",
    "\n",
    "with the **residual**\n",
    "\n",
    "$$\n",
    "\\mathbf{r} = A \\delta \\mathbf{x} = A \\hat{\\mathbf{x}} - \\mathbf{b} \n",
    "$$\n",
    "\n",
    "After calculating the LU decomposition with a **direct method** we know\n",
    "- $A$\n",
    "- $LU$ (not exactly the same as $A$ because of numerical errors!)\n",
    "- $\\mathbf{b}$\n",
    "- $\\hat{\\mathbf{x}}$ (our numerical non-exact solution)  \n",
    "\n",
    "Algorithm for iterative improvement:\n",
    "\n",
    "- compute $\\mathbf{r} = A \\hat{\\mathbf{x}} - \\mathbf{b}$\n",
    "- calculate $\\delta \\mathbf{x}$ as solution of $A \\delta \\mathbf{x} = \\mathbf{r}$\n",
    "  (since we already know $A = LU$ this is $\\mathcal{O}(N^2)$)\n",
    "- compute the improved solution\n",
    "\n",
    "  $$ \n",
    "  \\mathbf{x}_{\\mathrm{new}} = \\hat{\\mathbf{x}} - \\delta \\mathbf{x}\n",
    "  $$\n",
    "  \n",
    "- this can be repeated\n",
    "- (there are also iterative methods to improve the LU decomposition itself => not covered here)\n",
    "\n",
    "Additional cost:\n",
    "\n",
    "- the residue $\\mathbf{r}=A \\hat{\\mathbf{x}} - \\mathbf{b}$ has to be calculated\n",
    "- both $A$ and $LU$ have to be kept in memory\n",
    "- additional computation time $\\mathcal{O}(N^2)$ per iteration step is negligible compared to the $\\mathcal{O}(N^3)$ of the direct method (for sufficiently large matrices)\n",
    "- iterative methods are useful for very large $N$ and if we can already find a good approximation of $\\mathbf{x}$ with  $k \\ll N$ iterations (only $k N^2 \\ll N^3$ FLOPs)\n",
    "\n",
    "\n",
    "### Sherman-Morrison formula\n",
    "\n",
    "- suppose you already have obtained $A^{-1}$ of a square matrix $A$, after $\\mathcal{O}(N^3)$ operations\n",
    "- now you want to make a small change in $A$ (change one element, or one row, or one column)\n",
    "\n",
    "Do you need to start over and spend another $\\mathcal{O}(N^3)$ operations?\n",
    "\n",
    "> No, if the change in the matrix is of the form\n",
    "\n",
    "$$\n",
    "A'=A+u\\otimes v,\n",
    "$$\n",
    "\n",
    "where $u\\otimes v$ is a matrix whose ($i,j$) element is the product $u_iv_j$:\n",
    "\n",
    "$$\n",
    "u\\otimes v = \\begin{pmatrix}u_1\\\\u_2\\\\ \\vdots\\\\ u_N \\end{pmatrix}\\begin{pmatrix}v_1& v_2& \\ldots&v_N \\end{pmatrix}=\n",
    "\\begin{pmatrix} u_1v_1 & \\ldots & u_1v_N\\\\\n",
    "\\vdots & & \\vdots\\\\\n",
    "u_Nv_1 & \\ldots & u_Nv_N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Sherman-Morrison formula:\n",
    "\n",
    "$$\n",
    "(A+u\\otimes v)^{-1}=A^{-1}-\\frac{(A^{-1}u)\\otimes(v\\cdot A^{-1})}{1+v\\cdot A^{-1}u}\n",
    "$$\n",
    "\n",
    "Derivation (see \"Numerical Recipes\" books):\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "(A+u\\otimes v)^{-1}\t&=& (1+A^{-1}\\cdot u\\otimes v)^{-1}\\cdot A^{-1}\\\\\n",
    "\t\t\t&=& (1-A^{-1}\\cdot u\\otimes v+A^{-1}\\cdot u\\otimes v\\cdot A^{-1}\\cdot u\\otimes v\\mp\\ldots)\\cdot A^{-1}\\\\\n",
    "\t\t\t&=& A^{-1}-A^{-1}\\cdot u\\otimes v\\cdot A^{-1}+A^{-1}\\cdot u\\otimes v\\cdot A^{-1}\\cdot u\\otimes v\\cdot A^{-1}\\mp\\ldots\\\\\n",
    "\t\t\t&=& A^{-1}-A^{-1}\\cdot u\\otimes v\\cdot A^{-1}(1-\\Lambda+\\Lambda^2\\mp\\ldots)\\\\\n",
    "\t\t\t&=& A^{-1}-\\frac{(A^{-1}\\cdot u)\\otimes(v\\cdot A^{-1})}{1+\\Lambda},\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where \n",
    "- $\\Lambda\\equiv v\\cdot(A^{-1}u)$;\n",
    "- Taylor expansion was used in the second line;\n",
    "- the scalars $\\Lambda$ were factored out in line 4;\n",
    "- the series was written as $(1+\\Lambda)^{-1}$ using Taylor expansion in the last line.\n",
    "\n",
    "Complexity: $3N^2$ multiplications and $3N^2$ additions (even faster if $\\mathbf{u}$ or $\\mathbf{v}$ are unit vectors)\n",
    "\n",
    "Uses:\n",
    "- classes of sparse matrices\n",
    "- changing matrices one row/column at a time (do not use if you need to change *every* row!)\n",
    "- adding or removing one row/column\n",
    "\n",
    "### Inversion by partitioning\n",
    "\n",
    "Inverse of a partitioned square matrix $B$,\n",
    "\n",
    "$$\n",
    "B=\\left(\\begin{array}{cc}P&Q\\\\R&S\\end{array}\\right),\n",
    "$$\n",
    "\n",
    "has the form\n",
    "\n",
    "$$\n",
    "B^{-1}=\\left(\\begin{array}{cc}\\tilde{{P}}&\\tilde{{Q}}\\\\\\tilde{{R}}&\\tilde{{S}}\\end{array}\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\tilde{P}&=&P^{-1}+({P}^{-1}\\cdot{Q})\\cdot({S}-{R}\\cdot{P}^{-1}\\cdot Q)^{-1}\\cdot({R}\\cdot P^{-1}),\\\\\n",
    "\\tilde{Q}&=&-({P}^{-1}\\cdot{Q})\\cdot({S}-{R}\\cdot{P}^{-1}\\cdot{Q})^{-1},\\\\\n",
    "\\tilde{R}&=&-({S}-{R}\\cdot{P}^{-1}\\cdot{Q})^{-1}\\cdot({R}\\cdot{P}^{-1}),\\\\\n",
    "\\tilde{S}&=&({S}-{R}\\cdot{P}^{-1}\\cdot Q)^{-1}.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "(check by multiplying $B$ with its inverse)\n",
    "\n",
    "Determinant of a partitioned matrix:\n",
    "\n",
    "$$\n",
    "\\det B=\\det{P}\\det({S}-{R}\\cdot{P}^{-1}\\cdot{Q})=\\det{S}\\det({P}-{Q}\\cdot{S}^{-1}\\cdot{R})\n",
    "$$\n",
    "\n",
    "### QR decomposition\n",
    "\n",
    "$$\n",
    "A = Q\\cdot R\n",
    "$$\n",
    "\n",
    "where $R$ is upper triangular and $Q$ is orthogonal: $Q^T\\cdot Q=\\mathbf{1}$\n",
    "\n",
    "- used to solve linear least squares => later in the lecture\n",
    "- basis of QR eigenvalue algorithm\n",
    "\n",
    "Computed via\n",
    "\n",
    "- Gram-Schmidt orthonormalization (numerically unstable)\n",
    "- Householder transformations (mirror transformations)\n",
    "- Givens rotations (more complicated to implement, but well parallelizable)\n",
    "\n",
    "Can be used to solve linear equations, but needs twice as many operations as LU decomposition. However:\n",
    "\n",
    "- LU decomposition is hard to update (=> Sherman-Morrison) because of pivoting\n",
    "- QR decomposition on the other hand can be updated with a type of Sherman-Morrison formula\n",
    "- when solving many similar linear systems QR decomposition can be better\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "- powerful method for singular or near-singular systems\n",
    "- can \"diagnose\" problems with LU decomposition\n",
    "- needed for data analysis (linear least-squares) => later in the lecture\n",
    "- useful for \"inverting\" non-square matrices, determining the matrix rank etc.\n",
    "\n",
    "Any $M\\times N$ matrix $A$ (no matter how singular!) can be written as\n",
    "\n",
    "$$A=UDV^T$$\n",
    "\n",
    "with \n",
    "- an $M\\times N$ column orthogonal matrix $U$,\n",
    "- an $N\\times N$ diagonal matrix $D$ (diagonal elements $d_{i}\\geq0$ are the **singular values**)\n",
    "- an $N\\times N$ orthogonal matrix $V^T$\n",
    "\n",
    "Schematically\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "& & & & \\\\\n",
    "& &   & & \\\\\n",
    "& & {A}   & & \\\\\n",
    " & &  & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & &\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& &   & & \\\\\n",
    "& & {U}  & & \\\\\n",
    " & &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & &\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\!d_1\\! &  & & \\\\\n",
    " &\\!d_2\\!  & & \\\\\n",
    " &   &\\!\\ddots\\! & \\\\\n",
    "  &   & &\\!d_N\\!\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& &\\!{V}^T\\!  & & \\\\\n",
    "& &   & & \\\\\n",
    " & &   & & \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Orthogonality conditions, schematically:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "& & &  & & & & & \\\\\n",
    "& & &  & {U}^T & & & \\\\\n",
    " & & & & & & & & \\\\\n",
    "& &  & & & &\n",
    "\\end{pmatrix}\\cdot\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& &   & & \\\\\n",
    "& & {U}  & & \\\\\n",
    " & &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & & \\\\\n",
    "& &   & &\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& &\\!{V}^T\\! & & \\\\\n",
    "& &   & & \\\\\n",
    " & &   & & \n",
    "\\end{pmatrix}\\cdot\\begin{pmatrix}\n",
    "& &  & & \\\\\n",
    "& & {V}  & & \\\\\n",
    "& &   & & \\\\\n",
    " & &   & & \n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\! 1&  & & \\\\\n",
    " & \\!1  & & \\\\\n",
    " &   &\\! \\ddots& \\\\\n",
    " &   & &\\!1 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Visualization of SVD in 2D:\n",
    "- start from disc with 2 unit vectors\n",
    "- in this example, the original matrix distorts the disc to an ellipse (a matrix is a *linear transformation*)\n",
    "- SVD decomposes the matrix into three simple transformations: \n",
    "     - an initial rotation $V^‚àó$ (in our notation this is $V^T$), \n",
    "     - a scaling $\\Sigma$ along the coordinate axes (in our notation this is $D$)\n",
    "     - a final rotation U\n",
    "- The lengths $\\sigma_1$ and $\\sigma_2$ of the semi-axes of the ellipse are the singular values\n",
    "\n",
    "![SVD illustration](images/Singular_value_decomposition.gif)\n",
    "\n",
    "Image from [Wikipedia](https://en.wikipedia.org/wiki/Singular-value_decomposition), their notation is $M=U\\Sigma V^*$ instead of $A=UDV^T$ and $\\sigma_i$ instead of $d_i$\n",
    "\n",
    "<img src=\"images/Singular_value_decomposition.png\" alt=\"SVD illustration\" align=\"center\"  style=\"width: 500px;\"/>\n",
    "\n",
    "**SVD**\n",
    "- can be done for matrices of any shape\n",
    "- is almost unique\n",
    "    - up to making the same permutation of the columns of $U$, elements of $D$, and columns of $V$\n",
    "    - up to forming linear combinations of any columns of $U$ and $V$ whose corresponding elements of $D$ happen to be equal\n",
    "- is very stable numerically\n",
    "- allows us to easily pick a \"representative\" solution (the one with the smallest length) when there are infinitely many<br>=> needed for data analysis!\n",
    "- allows us to find an \"almost\" solution when there are none<br>=> needed for data analysis!\n",
    "- we will not discuss the algorithm, but it is part of all major libraries\n",
    "\n",
    "**Matrix inversion (of a square matrix) with SVD**:\n",
    "- for square matrices, $U$ is also square; $U$, $D$, $V$ have the same size\n",
    "- inverse becomes easy:\n",
    "\n",
    "$$\n",
    "A = U\\cdot[{\\rm diag}(d_i)]\\cdot V^T \\ \\ \\Rightarrow\\ \\ A^{-1}=V\\cdot[{\\rm diag}(1/d_i)]\\cdot U^T\n",
    "$$\n",
    "\n",
    "- solution of linear equations becomes easy:\n",
    "\n",
    "$$A\\mathbf{x}=\\mathbf{b}\\Rightarrow \\mathbf{x}=V\\cdot[{\\rm diag}(1/d_i)]\\cdot (U^T\\mathbf{b})$$\n",
    "\n",
    "- problems if some of the $d_i$ are zero (or close to zero...)\n",
    "- the *condition number* (see previous lecture) is defined as: max$(|d_i|)/$min$(|d_i|)$\n",
    "     - condition number infinite => matrix singular\n",
    "     - condition number too large ($\\gtrsim 1/\\varepsilon_m$) => matrix ill-conditioned\n",
    "     - in these cases we set (somewhat paradoxically) $1/d_i=0$ (remove singular values)\n",
    "\n",
    "A nonsingular matrix $A$ maps a vector space into one of the same dimension:\n",
    "\n",
    "![non-singular matrices](images/NRSVD_nonsing.png)\n",
    "\n",
    "**Singular matrices**:\n",
    "- if A is singular, then there is a subspace of $\\mathbf{x}$ (**nullspace**) so that $A\\mathbf{x}=0$\n",
    "- there is also some subspace of $\\mathbf{b}$ (**range of $A$**) that can be reached by $A$: i.e. there exists $\\mathbf{x}$ such that $A\\mathbf{x}=\\mathbf{b}$\n",
    "- the dimension of the range is the **rank** of $A$ (remember that for non-singular matrices the rank is $N$)\n",
    "- for singular matrices the rank is smaller than $N$ and the nullspace has dimension greater than zero\n",
    "\n",
    "> SVD explicitly constructs orthonormal bases for the *nullspace* and *range* of the matrix\n",
    "\n",
    "- columns of $U$ belonging to $d_i\\neq0$ span the range (orthonormal basis)\n",
    "- columns of $V$ belonging to $d_i=0$ span the nullspace (orthonormal basis)\n",
    "- solutions of $A\\mathbf{x}=0$ are defined by the nullspace basis => read out from $V$\n",
    "- solutions of $A\\mathbf{x}=\\mathbf{b}\\neq0$:\n",
    "    - if $\\mathbf{b}$ is not in the range of $A$ => no solution\n",
    "    - if $\\mathbf{b}$ is in the range of $A$ => solution exists (and any linear combination of nullspace vectors can be added to it)\n",
    "    \n",
    "A singular matrix $A$ maps a vector space into one of lower dimensionality (the range of $A$): \n",
    "    \n",
    "![SVD of singular matrices](images/NRSVD_sing.png)\n",
    "    \n",
    "- the nullspace of $A$ is mapped to zero\n",
    "- the solutions of $A\\cdot\\mathbf{x} = \\mathbf{d}$ consist of any one particular solution plus any vector in the nullspace\n",
    "- SVD selects the particular solution closest to zero\n",
    "- the point $\\mathbf{c}$ lies outside of the range of $A$ ($A\\cdot\\mathbf{x} = \\mathbf{c}$ has no solution)\n",
    "- SVD finds the best \"compromise solution\", namely a solution of $A\\cdot\\mathbf{x} = \\mathbf{c}'$ => see \"linear least-squares\" later\n",
    "\n",
    "### Column and row major\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "How is a matrix stored in computer memory?\n",
    "\n",
    "- row major (C, C++, Python): $\\ \\ \\ \\ $ contiguously in memory as ```1 2 3 4 5 6```\n",
    "- column major (Fortran, Matlab): contiguously in memory as ```1 4 2 5 3 6```\n",
    "\n",
    "(this generalizes to higher dimensions)\n",
    "\n",
    "### Linear algebra libraries\n",
    "\n",
    "Equally important:\n",
    "- small number of operations\n",
    "- small memory requirement\n",
    "- clever memory read-out\n",
    "\n",
    "A basis for many modern libraries is the Basic Linear Algebra Subsystem ([BLAS](http://www.netlib.org/blas/)):\n",
    "- 3 levels:\n",
    "    - level 1: scalar, scalar-vector and vector-vector\n",
    "    - level 2: matrix-vector\n",
    "    - level 3: matrix-matrix\n",
    "- huge increase in speed possible\n",
    "- free implementations: MKL, ATLAS,...\n",
    "- there are also machine specific optimized BLAS libraries (e.g. AMD: ACML, Apple: Accelerate, Intel: MKL)\n",
    "- here is the [documentation](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms)\n",
    "\n",
    "Some libraries and modules:\n",
    "- [LAPACK](http://www.netlib.org/lapack/) uses BLAS, especially Level 3\n",
    "- [GSL](https://www.gnu.org/software/gsl/) uses BLAS\n",
    "- Mathematica, Matlab and Octave use BLAS\n",
    "- NumPy and [SciPy](https://docs.scipy.org/doc/scipy/reference/linalg.html) use BLAS\n",
    "- you can also call BLAS functions in SciPy directly\n",
    "\n",
    "```scipy.linalg``` vs ```numpy.linalg```:\n",
    "- ```scipy.linalg``` contains all functions in ```numpy.linalg``` plus some more advanced ones\n",
    "- ```scipy.linalg``` is always compiled with BLAS/LAPACK support, while for NumPy this is optional<br> => SciPy might be faster depending on how your NumPy was installed\n",
    "\n",
    "> Use ```scipy.linalg``` unless you don't want to import ```scipy```\n",
    "\n",
    "Implementation on GPUs (graphics processing units)\n",
    "- signficant improvement in GPUs, thanks to the gaming industry\n",
    "- multicore GPUs are now affordable\n",
    "- massively parallel\n",
    "- libraries specific for GPUs (cuBLAS, Magma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.linalg as LA\n",
    "\n",
    "myMatrix = scipy.array([[1., 2.], [3., 4.]])\n",
    "myInvMatrix = LA.inv(myMatrix)\n",
    "print(myInvMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 0.0000000e+00]\n",
      " [8.8817842e-16 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(scipy.dot(myMatrix, myInvMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.around(scipy.dot(myMatrix, myInvMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.det(myMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0000000000000013"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1./LA.det(myInvMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.85892947-0.41963151j, -1.36745188+0.15815086j,\n",
       "         0.77412713+1.69980048j,  0.35429503-0.6534544j ,\n",
       "         0.26025711+1.01148851j],\n",
       "       [ 0.03757626-0.43064671j,  1.50268845-1.70888921j,\n",
       "         0.03250573-0.96992801j,  2.01194838+0.55773684j,\n",
       "        -0.58813105+0.41700299j],\n",
       "       [-2.0848362 +0.7576141j , -1.19080974-0.90352191j,\n",
       "         1.27829808+0.53009j   ,  2.12501411-1.32698354j,\n",
       "        -0.97281342+0.43931866j],\n",
       "       [ 1.86450351-0.36182246j, -0.96730906+0.30861901j,\n",
       "         0.37569513-2.39960954j,  0.4237144 +0.57683613j,\n",
       "         1.18512586+1.09576293j],\n",
       "       [ 2.43577561+1.1520762j ,  0.55215916+1.08061376j,\n",
       "        -0.25474526+1.57588748j, -1.13956064+1.17906452j,\n",
       "        -0.82017069+0.59983885j],\n",
       "       [ 1.42485067+2.48962682j, -0.22983815+1.1072615j ,\n",
       "         2.00736635-1.47809547j,  0.06288505+0.22943667j,\n",
       "        -2.47592698-0.82358964j],\n",
       "       [-0.78732941-1.02811623j, -1.15486985+0.41721049j,\n",
       "         0.47523286+0.30381578j,  1.54956229-0.81164847j,\n",
       "        -1.22569109-0.3493684j ],\n",
       "       [ 1.89351113+0.5712192j ,  0.93814538-1.17142341j,\n",
       "        -1.26041992-1.0085945j ,  1.38748891-1.25656893j,\n",
       "        -0.20876983-0.79393569j],\n",
       "       [-0.70207747-0.73543467j,  0.23456847-1.49866197j,\n",
       "        -1.07585305-1.11569518j, -0.72490042+0.87428653j,\n",
       "         1.48039706-0.91141315j]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Singular Value Decomposition\n",
    "m, n = 9, 5\n",
    "a = scipy.random.randn(m, n) + 1.j*scipy.random.randn(m, n)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.26122327+0.03686052j, -0.05687967+0.06268343j,\n",
       "          0.08722056-0.39214653j, -0.13223121+0.00993957j,\n",
       "         -0.0878773 -0.26310947j,  0.51077867-0.39078752j,\n",
       "         -0.10246714+0.03899696j,  0.34383674-0.27022293j,\n",
       "         -0.00433383+0.22455965j],\n",
       "        [ 0.09383014-0.21199077j, -0.24661407-0.42603546j,\n",
       "         -0.0633234 +0.08639314j,  0.21566015-0.24290874j,\n",
       "          0.09656165-0.04956588j,  0.20333456+0.07707197j,\n",
       "          0.18375041+0.08819359j, -0.28312972-0.60048707j,\n",
       "         -0.20503882-0.05955802j],\n",
       "        [-0.4042696 +0.12852727j, -0.40249632-0.27204704j,\n",
       "         -0.01771479-0.08913282j,  0.06235376-0.07282562j,\n",
       "          0.05485501-0.15947323j, -0.11779542+0.20481829j,\n",
       "         -0.24041152+0.2363504j ,  0.01969783+0.08535063j,\n",
       "          0.55305991-0.2347544j ],\n",
       "        [ 0.33967575-0.01888453j, -0.2486293 +0.04152524j,\n",
       "         -0.23070798-0.09108827j, -0.60598129+0.21021226j,\n",
       "          0.13988484-0.19766965j, -0.12100528+0.34146871j,\n",
       "         -0.10753844-0.20504663j,  0.06417267-0.25726892j,\n",
       "          0.08967593+0.16111196j],\n",
       "        [ 0.26761774+0.1711479j ,  0.34484   +0.01945609j,\n",
       "          0.48723212-0.08976596j, -0.03245747-0.12645307j,\n",
       "         -0.1194899 -0.35461841j,  0.0755413 +0.10844449j,\n",
       "          0.28565152-0.0628158j , -0.24530505-0.10471234j,\n",
       "          0.4488872 -0.04023334j],\n",
       "        [ 0.20478605+0.51662259j, -0.09043132-0.34127235j,\n",
       "          0.1495529 +0.22664127j, -0.12434309+0.21556503j,\n",
       "          0.08733078+0.39857788j,  0.40208908-0.12188793j,\n",
       "         -0.21180713-0.04592731j, -0.14651494+0.07898057j,\n",
       "          0.07437195+0.08733097j],\n",
       "        [-0.22563445-0.0147634j , -0.2221013 -0.1575102j ,\n",
       "          0.13296297-0.10432656j,  0.00102053+0.50844743j,\n",
       "         -0.00476505+0.16083656j, -0.07601164+0.04091063j,\n",
       "          0.72345689-0.05558428j,  0.12159112+0.03624037j,\n",
       "          0.04017789+0.10057639j],\n",
       "        [ 0.18596558-0.115764j  , -0.32218729-0.07060767j,\n",
       "          0.17570325+0.44609932j,  0.05246826-0.12935647j,\n",
       "         -0.60773573-0.03276355j, -0.11046667-0.10797389j,\n",
       "         -0.0329578 -0.1764738j ,  0.37997727-0.03943742j,\n",
       "          0.11007446+0.09644734j],\n",
       "        [ 0.0072217 -0.26843532j,  0.11092031+0.10856962j,\n",
       "         -0.33287374+0.26254253j, -0.19448573-0.24800035j,\n",
       "          0.25454418+0.23783431j,  0.30370228-0.18571219j,\n",
       "          0.28050547-0.09624616j,  0.16910292+0.02356125j,\n",
       "          0.47740636-0.1778788j ]]),\n",
       " array([7.14775907, 5.39046759, 4.70412014, 3.19889044, 2.35194197]),\n",
       " array([[ 0.69704181+0.j        ,  0.38100705+0.16006161j,\n",
       "         -0.04130576-0.46301835j, -0.15229084+0.22014041j,\n",
       "          0.02771759+0.23416214j],\n",
       "        [-0.04382622+0.j        ,  0.20637432+0.24157817j,\n",
       "         -0.00647298+0.45019226j, -0.44239755+0.59699477j,\n",
       "          0.14627884-0.34770401j],\n",
       "        [ 0.44932676+0.j        , -0.16265588-0.05352773j,\n",
       "         -0.31843649+0.51032831j, -0.0035323 -0.08571841j,\n",
       "         -0.62831854+0.06929802j],\n",
       "        [-0.3793064 +0.j        ,  0.75482775+0.25201961j,\n",
       "         -0.18376752+0.0713217j ,  0.00520332-0.28889307j,\n",
       "         -0.28162647+0.1455706j ],\n",
       "        [-0.40797561+0.j        , -0.25213125-0.04574272j,\n",
       "         -0.24973551-0.34370187j, -0.22139871+0.48617236j,\n",
       "         -0.39852416+0.37840728j]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.svd(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 9), (5,), (5, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, s, Vh = LA.svd(a)\n",
    "U.shape,  s.shape, Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.85892947-0.41963151j, -1.36745188+0.15815086j,\n",
       "         0.77412713+1.69980048j,  0.35429503-0.6534544j ,\n",
       "         0.26025711+1.01148851j],\n",
       "       [ 0.03757626-0.43064671j,  1.50268845-1.70888921j,\n",
       "         0.03250573-0.96992801j,  2.01194838+0.55773684j,\n",
       "        -0.58813105+0.41700299j],\n",
       "       [-2.0848362 +0.7576141j , -1.19080974-0.90352191j,\n",
       "         1.27829808+0.53009j   ,  2.12501411-1.32698354j,\n",
       "        -0.97281342+0.43931866j],\n",
       "       [ 1.86450351-0.36182246j, -0.96730906+0.30861901j,\n",
       "         0.37569513-2.39960954j,  0.4237144 +0.57683613j,\n",
       "         1.18512586+1.09576293j],\n",
       "       [ 2.43577561+1.1520762j ,  0.55215916+1.08061376j,\n",
       "        -0.25474526+1.57588748j, -1.13956064+1.17906452j,\n",
       "        -0.82017069+0.59983885j],\n",
       "       [ 1.42485067+2.48962682j, -0.22983815+1.1072615j ,\n",
       "         2.00736635-1.47809547j,  0.06288505+0.22943667j,\n",
       "        -2.47592698-0.82358964j],\n",
       "       [-0.78732941-1.02811623j, -1.15486985+0.41721049j,\n",
       "         0.47523286+0.30381578j,  1.54956229-0.81164847j,\n",
       "        -1.22569109-0.3493684j ],\n",
       "       [ 1.89351113+0.5712192j ,  0.93814538-1.17142341j,\n",
       "        -1.26041992-1.0085945j ,  1.38748891-1.25656893j,\n",
       "        -0.20876983-0.79393569j],\n",
       "       [-0.70207747-0.73543467j,  0.23456847-1.49866197j,\n",
       "        -1.07585305-1.11569518j, -0.72490042+0.87428653j,\n",
       "         1.48039706-0.91141315j]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstruct the original matrix from the decomposition:\n",
    "sigma = scipy.zeros((m, n))\n",
    "for i in range(min(m, n)):\n",
    "    sigma[i, i] = s[i]\n",
    "a1 = scipy.dot(U, scipy.dot(sigma, Vh))\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.allclose(a, a1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
